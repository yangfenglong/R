---
title: "models_regression"
output: html_notebook
---

gam 
============

https://www.r-bloggers.com/generalized-additive-models/
This is also a flexible and smooth technique which captures the Non linearities in the data and helps us to fit Non linear Models.In this article I am going to discuss the implementation of GAMs in R using the 'gam' package .Simply saying GAMs are just a Generalized version of Linear Models in which the Predictors \(X_i\) depend Linearly or Non linearly on some Smooth Non Linear functions like Splines , Polynomials or Step functions etc.

The Regression Function \(F(x) \) gets modified in Generalized Additive Models , and only due to this transformation the GAMs are better in terms of Generalization to random unseen data , fits the data very smoothly and flexibly without adding Complexities or much variance to the Model most of the times.

The basic idea in Splines is that we are going to fit Smooth Non linear Functions on a bunch of Predic tors \(X_i\) . Additive in the name means we are going to fit and retain the additivity of the Linear Models.

Conclusion

Generalized Additive Models are a very nice and effective way of fitting Non linear Models which are smooth and flexible.Best part is that they lead to interpretable Models. We can easily mix terms in GAMs,some linear and some Non Linear terms and then compare those Models using the anova() function which **performs a Anova test for goodness of fit**.The non linear terms on Predictors \(X_i\) can be anything from smoothing splines , natural cubic splines to polynomial functions or step functions etc. GAMs are additive in nature , which means there are no interaction terms in the Model.

Thanks a lot for reading the article,and make sure to like and share it.Cheers!

Generalized additive models with integrated smoothness estimation
传统数据分析中，线性方法是一类很重要的方法，它们可以提供对数据的预测，并给出输出和输入之间的关系，但在很多现实问题中，线性方法却往往不能很好地工作，因为输入和输出之间的关系往往是非线性的，对此我们可以利用Basis Expansion的方法来解决，除此之外也可以采用一种更灵活的方法，即Generalized Additive Model.

```{r}
library("mgcv")
set.seed(2) ## simulate some data... 
dat <- gamSim(1,n=400,dist="normal",scale=2)
b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
summary(b)
plot(b,pages=1,residuals=TRUE)  ## show partial residuals
plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs
## run some basic model checks, including checking
## smoothing basis dimensions...
gam.check(b)

```




Fit a Smoothing Spline
======
Description
http://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html
smooth.spline {stats}	R Documentation

Fits a cubic smoothing spline to the supplied data.
```{r}
##-- artificial example
y18 <- c(1:3, 5, 4, 7:3, 2*(2:5), rep(10, 4))
xx  <- seq(1, length(y18), len = 201)
(s2   <- smooth.spline(y18)) # GCV
(s02  <- smooth.spline(y18, spar = 0.2))
(s02. <- smooth.spline(y18, spar = 0.2, cv = NA))
plot(y18, main = deparse(s2$call), col.main = 2)
lines(s2, col = "gray"); lines(predict(s2, xx), col = 2)
lines(predict(s02, xx), col = 3); 
mtext(deparse(s02$call), col = 3)

## Specifying 'lambda' instead of usual spar :
(s2. <- smooth.spline(y18, lambda = s2$lambda, tol = s2$tol))



## The following shows the problematic behavior of 'spar' searching:
(s2  <- smooth.spline(y18, 
                      control = list(trace = TRUE, tol = 1e-6, low = -1.5)))
(s2m <- smooth.spline(y18, 
                      cv = TRUE, 
                      control = list(trace = TRUE, tol = 1e-6, low = -1.5)))
## both above do quite similarly (Df = 8.5 +- 0.2)
```


SEM 
===================
```{r}
library(MASS)
library(sem)
library(dplyr)
# 随机生成数据
dat <- matrix(rnorm(100), 25, 4) # 25*4 矩阵
colnames(dat) <- c('a', 'b', 'c', 'd') # 分配列名
cor_num <- cor(dat) # 计算相关性矩阵

# 构建模型
# 我们假设 变量a 通过b, c 影响 d
  model.kerch <- specifyModel(
    # sem包是以文本的形式提交, 路径关系
    # 默认所有变量都是观测变量
    # 第一列表示路径
    #第二列表示回归系数,可以先假定一个任意变量, 模型随后进行修正赋值; 第三列为自由参数的起始值, 没有可设置为NA sem会计算起始值
    # 下面的变量, 如果存在于数据中, 则为观测变量, 否则为潜在变量
    text = "a -> b, a_b, NA
      a -> c, a_c, NA
      a -> d, a_d, NA
      b -> d, b_d, NA
      b -> c, b_c, NA
      c -> d, c_d, NA
      a <-> a, a_a, NA
      b <-> b, b_b, NA
      c <-> c, c_c, NA
      d <-> d, d_d, NA
    ")
  
  # 最后四行表示变量自身的影响
  # 构建sem
  # 第二个参数可以直接使用相关性矩阵,或者协方差矩阵
  out_sem <- sem(model.kerch, cor_num, nrow(dat))
  # 回归系数
  coef <- out_sem$coeff
  # 系数名
  coeff_name <- out_sem$semmod[,1]
  summary(out_sem)
   # 构建路径图
  pathDiagram(out_sem, edge.labels="values")
  
  
  
  
  
  
  
a <- c("a","b","c","d")
b <- c("a","d")
ifelse(a %in% b, which(y==z), 0) 
##若y的值包含在z里面，输出y==z的位置，否者输出0
# [1] 0 2 0 4

```

pls_pm
===

#inner model 下三角是數字

	Rec_time	Plant	Soil_phc	Aggregate	Microbiome
Rec_time	0	0	0	0	0
Plant	1	0	0	0	0
Soil_phc	0	0	0	0	0
Aggregate	1	1	1	0	0
Microbiome	1	1	0	0	0

# new list of blocks (with names of variables) 
new_blocks_str = list( 
c("GSH", "GSA", "SSH", "SSA"), 
c("NGCH", "NGCA", "CSH", "CSA"), 
c("WMH", "WMA", "LWR", "LRWL"))

箭头末端排在最下行

The way in which you should read this matrix is by “columns affecting rows”. 


```{r}
# load package plspm 
library(plspm)

# 1 data
data<-read.table("clipboard",sep="\t",header=T)

# 2 path matrix (inner model)

in_model <- read.table("clipboard",head=T,row.names = 1) 
in_model<-as.matrix(in_model)
# plot the path matrix 
# innerplot(in_model)

# 3  blocks of indicators (outer model) 
blocks<-list(1,2:6, 10:24, 7:9, 25:31)
modes = c("A", "A", "A", "A", "A") # vector of modes (reflective)

# run plspm analysis 
# plspm(Data, path matrix, blocks, modes = NULL)
pls = plspm(data, in_model, blocks, modes = modes) 



# plotting results (inner model) 
plot(pls,what = "inner")

# plotting loadings of the outer model 
plot(pls, what = "loadings", 
     arr.width = 0.05,
     cex.txt = 0.8, box.cex = 1,txt.col = "black"
     )

# Besides checking the loadings of the indicators with their own latent variables, we must also check the so-called cross-loadings. That is, the loadings of an indicator with the rest of latent variables. 

# load ggplot2 and reshape 
library(tidyverse)
# reshape crossloadings data.frame for ggplot 
pls$crossloadings %>% 
  gather(key = "blocks",value="vars",-c(name,block)) %>%
  unite(col="crossloadings",c(block,blocks),sep=",") %>%
  # bar-charts of crossloadings by block 
  ggplot(aes(x = name, y = vars, fill = crossloadings)) +
# add horizontal reference lines 
  geom_hline(yintercept = 0, color = "gray75") + 
  geom_hline(yintercept = 0.5, color = "gray70", linetype = 2) + 
  # indicate the use of car-charts 
  geom_bar(stat = 'identity', position = 'dodge') + 
  # panel display (i.e. faceting) 
  facet_wrap( ~ crossloadings) + 
  # tweaking some grahical elements 
  theme(axis.text.x = element_text(angle = 90,size = 5), 
        line = element_blank(), 
        plot.title = element_text(size = 12),
        legend.position = "none") +
# add title 
  ggtitle("Crossloadings") -> plt
 

# Structural Model Assessment
  ggsave("crossload.pdf",plt,width = 12,height = 7)

# inner model To inspect the results of each regression in the structural equations we
pls$inner_model

 
# Besides the results of the regression equations, the quality of the structural model is evaluated by examining three indices or quality metrics:
# • the R2 determination coefficients
# • the redundancy index
# • the Goodness-of-Fit (GoF)

# select R2
pls$inner_summary


# gof index 
#pls$gof
```



PLS Path Modeling 
==================

```{r}
# load package plspm 
library(plspm)

# load data spainfoot 
data(spainfoot) ## 1


# rows of the inner model matrix; rows of the path matrix  
Attack = c(0, 0, 0) 
Defense = c(0, 0, 0) 
Success = c(1, 1, 0)
# path matrix created by row binding; 
foot_path = rbind(Attack, Defense, Success) ## 2 path matrix (inner model)
# add column names (optional) 
colnames(foot_path) = rownames(foot_path)


# plot the path matrix 
innerplot(foot_path)


# The third ingredient is the outer model ## 3 blocks of indicators (outer model)
# In other words, we tell plspm() what variables of the data are associated with what latent variables.
foot_blocks = list(c(1:4), 5:8, 9:12)

# By default, plspm() assumes that the measurement of the latent variables is in reflective mode, known as mode A in the PLS-PM world. 
# all latent variables are measured in a reflective way 
foot_modes = c("A", "A", "A") # vector of modes (reflective)

# An alternative type of measurement is the formative mode, known as mode B. 
# Success in formative mode B 
foot_modes2 = c("A", "A", "B") # 

# run plspm analysis 
# plspm(Data, path matrix, blocks, modes = NULL)
foot_pls = plspm(spainfoot, foot_path, foot_blocks, modes = foot_modes) 
foot_pls2 = plspm(spainfoot, foot_path, foot_blocks, modes = foot_modes2)


# Success = b1Attack +b2Defense
foot_pls$inner_model

# plotting results (inner model) 
plot(foot_pls,what = "inner")
innerplot(foot_pls)
outerplot(foot_pls)
# plotting loadings of the outer model 
plot(foot_pls, what = "loadings", arr.width = 0.1)

# load ggplot2 and reshape 
library(tidyverse)
# reshape crossloadings data.frame for ggplot 
foot_pls$crossloadings %>% 
  gather(key = "blocks",value="vars",-c(name,block)) %>%
  unite(col="crossloadings",c(block,blocks),sep=",") %>%
  # bar-charts of crossloadings by block 
  ggplot(aes(x = name, y = vars, fill = crossloadings)) +
# add horizontal reference lines 
  geom_hline(yintercept = 0, color = "gray75") + 
  geom_hline(yintercept = 0.5, color = "gray70", linetype = 2) + 
  # indicate the use of car-charts 
  geom_bar(stat = 'identity', position = 'dodge') + 
  # panel display (i.e. faceting) 
  facet_wrap( ~ crossloadings) + 
  # tweaking some grahical elements 
  theme(axis.text.x = element_text(angle = 90), line = element_blank(), plot.title = element_text(size = 12)) +
# add title 
  ggtitle("Crossloadings")
  


```

Orthogonal Signal Correction Partial Least Squares (O-PLS) Discriminant Analysis (O-PLS-DA)
=============
https://github.com/dgrapov/TeachingDemos/blob/master/Demos/Predictive%20Modeling/Iris%20O-PLS-DA/O-PLS%20modeling%20of%20Iris%20data.md#prep

```{r}
## Preparation for modeling

data(iris)
tmp.data<-iris[,-5]
tmp.group<-iris[,5] # species
tmp.y<-matrix(as.numeric(tmp.group),ncol=1) 


source("http://pastebin.com/raw.php?i=UyDBTA57") 

train.test.index.main=test.train.split(nrow(tmp.data),n=1,strata=tmp.group,split.type="duplex",data=tmp.data)
train.id<-train.test.index.main=="train"

#partition data to get the trainning set
tmp.data<-tmp.data[train.id,]
tmp.group<-tmp.group[train.id]
tmp.y<-tmp.y[train.id,]

#the variables could be scaled now, or done internally in the model for each CV split (leave-one-out)
#scaled.data<-data.frame(scale(tmp.data,center=TRUE, scale=TRUE)) 
scaled.data<-tmp.data

```

